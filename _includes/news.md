<h2 style="margin: 60px 0px 10px;">News</h2>

<ul>
<li><strong>[June 2025]<sup><font color=red size=2>New</font></sup></strong> Our paper, "FocusX: All-in-Focus Image Synthesis for Dynamic Scenes on Mobile Devices", is accpeted by  <strong>MobiCom'25</strong>!</li>
<li><strong>[May 2025]<sup><font color=red size=2>New</font></sup></strong> Our paper, "迈向泛在智能：端侧大语言模型现状与展望", is accpeted by  <strong>《中国科学基金》</strong>!</li>
<li><strong>[Jan. 2025]<sup><font color=red size=2>New</font></sup></strong> Our paper, "端智能推理加速技术综述", is accpeted by  <strong>《电子学报》</strong>!</li>
<li><strong>[Dec. 2024]</strong> Our paper, "MobiFuse: A High-Precision On-device Depth Perception System with Multi-Data Fusion", is now published on  <strong>arXiv</strong>. All are welcome to join the discussion.</li>
<li><strong>[Nov. 2024]</strong> Our paper, "MAML-RAL: Learning Domain-Invariant HOI Rules for Real-Time Video Matting", is accepted by <strong>IEEE Transactions on Circuits and Systems for Video Technology</strong>! Congratulations to Jiang.</li>
<li><strong>[Oct. 2024]</strong> I was recognized as the 2024 ACM SIGAPP China Outstanding Doctoral Dissertation Award! </li>
<li><strong>[Sep. 2024]</strong> Our paper, "Heterogeneous Parallel Acceleration for Edge Intelligence Systems: Challenges and Solutions", is accepted by <strong>IEEE Consumer Electronics Magazine</strong>! </li>
<li><strong>[Sep. 2024]</strong> Our paper, "FL-Joint: Joint Aligning Features and Labels in Federated Learning for Data Heterogeneity", is accepted by <strong>Complex & Intelligent System</strong>! Congratulations to Wenxin. </li>
<li><strong>[Aug. 2024]</strong> Our paper, "Mobile Generative AI: Opportunities and Challenges", is accepted <strong>IEEE Wireless Communications</strong>! Congratulations to Ye. </li>
<li><strong>[July 2024]</strong> Our paper, "MultiCounter: Multiple Action Agnostic Repetition Counting in Untrimmed Videos", is accepted by <strong>ECAI 2024</strong>! Congratulations to Yin. </li>
<li><strong>[Mar. 2024]</strong> Our paper, "MBSeg: Real-Time Contour-Based Method Instance Segmentation for Egocentric Vision", is accepted by <strong>IEEE Internet of Things Journal(IoTJ)</strong>! Congratulations to Wei. </li>
<li><strong>[Dec. 2023]</strong> Finished my PhD with <strong>Outstanding Graduate Award</strong> from Central South University. </li>
<li><strong>[Aug. 2023]</strong> Our paper, "Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views", is accepted by <strong>ACM SIGGRAPH Asia 2023</strong>! Congratulations to Taeho Kang. </li>
<li><strong>[July 2023]</strong> Our paper, "HiMoDepth: Efficient Training-free High-resolution On-device Depth Perception", is accepted by <strong>IEEE Transactions on Mobile Computing</strong>. </li>
<li><strong>[Sep. 2022]</strong> I join the <a href="https://hcs.snu.ac.kr/">HCS Lab</a> as a visiting research at Seoul National University in Korea, advised by <a href="http://youngkilee.blogspot.com/">Prof. Youngki Lee</a>. </li>
<li><strong>[Aug. 2022]</strong> I will give a talk on Hunan Province "Big Data and Artificial Intelligence" Innovation Forum. </li>
<li><strong>[June 2022]</strong> Our paper, "MobiDepth: Real-Time Depth Estimation Using On-Device Dual Cameras", is accepted by <strong>ACM MobiCom'22</strong> ! </li>
<li> <a href="javascript:toggle_vis('newsmore')">Show more</a> </li>
<div id="newsmore" style="display:none"> 

<li><strong>[Jan. 2022]</strong> Our paper, "MVPose: Realtime Multi-Person Pose Estimation using Motion Vector on Mobile Devices", is accepted by <strong>IEEE Transactions on Mobile Computing</strong>. </li>
<li><strong>[Aug. 2021]</strong> I joined the Insitute for AI Industry Research(AIR), at Tsinghua University. Mentor: <a href="https://yunxinliu.github.io/">Prof.Yunxin Liu</a>. </li>
<li><strong>[Apr. 2021]</strong> Our paper," Optimizing Federated Learning on Device Heterogeneity with A Sampling Strategy", is accepted by <strong>IEEE IWQoS 2021</strong>.</li>
<li><strong>[Sep. 2020]</strong> Our paper, "MobiPose: Real-Time Multi-Person Pose Estimation on Mobile Devices", is accepted by <strong>ACM SenSys'20</strong> !</li>
</div>
</ul>
